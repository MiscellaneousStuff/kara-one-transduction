{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kara One - Transduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import KaraOneDataset\n",
    "\n",
    "dataset_dir = \"C:\\\\Users\\\\win8t\\\\OneDrive\\\\Desktop\\\\projects\\\\kara-one-transduction\\\\p\"\n",
    "dataset = KaraOneDataset(dataset_dir, end_idx=165, scale_data=False, start_idx=163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dataset[0][\"eeg_vocal\"].shape, dataset[0][\"eeg_vocal_raw\"].shape, dataset[0][\"eeg_vocal_feats\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Channel Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: [Paper](http://www.cs.toronto.edu/~complingweb/data/karaOne/ZhaoRudzicz15.pdf)\n",
    "\n",
    "Pearson correlation coefficients between audio features and imagined speech EEG features\n",
    "\n",
    "|Sensor | FC6    | FT8 |  C5 | CP3|  P3 |\n",
    "| - | - | - | - | - | - |\n",
    "|Mean r | 0.3781 | 0.3758 | 0.3728 | 0.3720 | 0.3696 |\n",
    "\n",
    "| Sensor | T7 | CP5 | C3 | CP1 |C4 |\n",
    "| - | - | - | - | - | - |\n",
    "Mean r | 0.3686|  0.3685|  0.3659| 0.3626 |0.3623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_eeg(eeg_type, target_channels, idx, feat=False):\n",
    "    plt.rcParams[\"figure.figsize\"] = (14, 9)\n",
    "\n",
    "    if not target_channels:\n",
    "        target_channels = dataset.eeg_data.ch_names\n",
    "    \n",
    "    print(\"target_channels:\", target_channels)\n",
    "    \n",
    "    print(dataset.eeg_data)\n",
    "\n",
    "    keep_idx_s = [dataset.eeg_data.ch_names.index(target_ch)\n",
    "                for target_ch in target_channels]\n",
    "\n",
    "    example = dataset[idx]\n",
    "    print(example[\"label\"])\n",
    "    eeg_data = example[eeg_type]\n",
    "    # eeg_data = eeg_data - eeg_data.mean(axis=1, keepdims=True)\n",
    "    print(\"eeg data shape:\", eeg_data.shape)\n",
    "\n",
    "    if feat:\n",
    "        eeg_data = np.asarray(np.split(eeg_data, 62))\n",
    "    \n",
    "    for idx in keep_idx_s:\n",
    "        print(\"idx:\", idx)\n",
    "        label = dataset.eeg_data.ch_names[idx]\n",
    "        if feat:\n",
    "            data = eeg_data[idx, 16]\n",
    "        else:\n",
    "            data = eeg_data[:135, (idx*5)+2]\n",
    "            # data = eeg_data[:, (idx*5)+2] # working (p_r, rms)\n",
    "        # print(eeg_data.shape)\n",
    "        # print(data.shape, eeg_data.shape, eeg_data.shape[1] / 1000, data)\n",
    "        print(\"CUR ELECTRODE DATA:\", data.shape)\n",
    "        plt.plot(data, label=label)\n",
    "        print(len(data), len(data / max(data)))\n",
    "\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "# target_channels = \"FC6 FT8 C5 CP3 P3 T7 CP5 C3 CP1 C4\".split(\" \") # top 10\n",
    "# target_channels = \"FC6 FT8 C5 CP3 CP5 C3 CP1 C4\".split(\" \") # top 10 - T7 (temporal 7?)\n",
    "# target_channels = target_channels[0:10]\n",
    "target_channels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eeg(\"eeg_vocal_feats\", target_channels, idx=0, feat=False)\n",
    "target_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]\n",
    "plt.plot(example[\"audio_raw\"])\n",
    "len(example[\"audio_raw\"]) / 16_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel Spectrogram of Audio (Vocalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "def plot_mel_spectrogram(mel_spec, title):\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    ax.set_title(f\"Mel Spectogram \\\"{title}\\\"\")\n",
    "    pred = np.swapaxes(mel_spec, 0, 1)\n",
    "    cax = ax.imshow(pred, interpolation='nearest', cmap=cm.coolwarm, origin='lower')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mel_spec = example[\"audio_feats\"]\n",
    "print(example_mel_spec.shape)\n",
    "a = plot_mel_spectrogram(example_mel_spec, example[\"label\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28453d1081d3c550fce4dd227bac61cebcdf565b50505afc80cae3c0cf61cf22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
